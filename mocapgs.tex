%% 
%% Copyright 2019-2024 Elsevier Ltd
%% 
%% This file is part of the 'CAS Bundle'.
%% --------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3c of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3c or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'CAS Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for cas-dc documentclass for 
%% double column output.

\documentclass[a4paper,fleqn]{cas-dc}

\usepackage[numbers]{natbib}

%% --------------------------------
%% | Own Commands                 |
%% --------------------------------
\usepackage{newtxtt}
\newcommand{\Vector}[1]{{\ensuremath{\textbf{\textit{#1}}}}}
\newcommand{\Matrix}[1]{{\ensuremath{\textbf{\texttt{#1}}}}}
\newcommand{\TMatrix}[3]{{^{\mathrm{#1}}\Matrix{#2}_{\mathrm{#3}}}}
\newcommand{\Quat}[1]{{\ensuremath{\textbf{#1}}}}
\newcommand{\DQuat}[1]{{\ensuremath{\hat{\Quat{#1}}}}}
\newcommand{\DVector}[1]{{\ensuremath{\hat{\Vector{#1}}}}}
\newcommand{\trans}{\ensuremath{^\top}}

% Author macros
\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}

\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}

% Short title
\shorttitle{MocapGS: Robust Online 3D Mapping Using Motion Capture and Gaussian Splatting}    

% Short author
\shortauthors{Theodor Kapler et al.}  

% Main title of the paper
\title [mode = title]{MocapGS: Robust Online 3D Mapping Using Motion Capture and Gaussian Splatting}  

% Authors
\author[1]{Theodor Kapler}%[<options>]
\cormark[1]
\ead{theodor.kapler@student.kit.edu}
\affiliation[1]{organization={Institute of Photogrammetry and Remote Sensing, Karlsruhe Institute of Technology},
            addressline={Englerstr. 7}, 
            city={Karlsruhe},
            postcode={76131}, 
            state={Baden-Württemberg},
            country={Germany}}

\author[1]{Markus Hillemann}[orcid=0000-0002-8906-0450]
\ead{markus.hillemann@kit.edu}

\author[1]{Robert Langendörfer}
\ead{robert.langendoerfer@kit.edu}

\author[1]{Markus Ulrich}[orcid=0000-0001-8457-5554]
\ead{markus.ulrich@kit.edu}

% Corresponding author text
\cortext[1]{Corresponding author}

% Abstract
\begin{abstract}
Image-based online 3D mapping allows scenes to be reconstructed incrementally, with new images continuously integrated to provide direct feedback on the reconstructed scene. However, many online reconstruction methods, typically SLAM-applications, rely on image-based camera pose estimation, which is prone to drift and loop-closure errors. To address this issue, we present MocapGS, an online 3D reconstruction method that directly integrates accurate camera poses obtained from a Motion Capture system directly into a 3D Gaussian Splatting-based reconstruction pipeline. By decoupling pose estimation from scene reconstruction, MocapGS leverages externally provided, metrically accurate camera poses to improve robustness and global consistency. To enable this integration, a complete workflow is developed, including temporal synchronization between the camera and the Motion Capture system, camera calibration, hand-eye calibration, and online reconstruction from sequential image-pose pairs. The proposed method is evaluated on multiple datasets and compared to methods estimating poses from image data. Experimental results show that MocapGS yields more stable camera trajectories and improved reconstruction quality compared to such methods, particularly with respect to global consistency. The results demonstrate that Motion Capture can serve as a reliable primary source of camera poses for online 3D reconstruction, highlighting the potential of Motion Capture-driven reconstruction pipelines.
\end{abstract}

% Keywords
\begin{keywords}
Online 3D Reconstruction \sep Motion Capture \sep Gaussian Splatting \sep Time Synchronization \sep Camera Calibration \sep Hand-eye Calibration
\end{keywords}

\maketitle

% Main text
\section{Introduction}\label{sec:introduction}
Image-based 3D reconstruction has become a fundamental technology in computer vision, photogrammetry, and robotics, with applications ranging from industrial inspection \cite{steger_machine_2018} over autonomous driving \cite{behley_efficient_2018} to digital heritage preservation \cite{gomes_3d_2014}. As cameras are inexpensive, flexible, and widely available, image-based approaches remain particularly attractive compared to active sensing modalities.

Classical 3D reconstruction pipelines are predominantly batch-based. Methods such as Structure from Motion (SfM) \cite{schonberger_structure--motion_2016} estimate camera poses, intrinsic parameters, and sparse geometry jointly from a complete set of images, followed by a densification step using Multi-view Stereo (MVS) \cite{schonberger_pixelwise_2016} or learning-based representations, like 3D Gaussian Splatting (3DGS) \cite{kerbl_3d_2023}. While these approaches can achieve good reconstruction quality, they fundamentally require all images to be available in advance. As a consequence, they are unsuitable for scenarios where image data is acquired sequentially and where immediate feedback is desired.

Incremental reconstruction approaches address these issues. Simultaneous Localization and Mapping (SLAM)-methods \cite{cadena_past_2016}  estimate camera poses and scene structure on-the-fly, enabling online operation and continuous map expansion as new images arrive. More recently, learning-based scene representations such as 3DGS have been adapted to online image acquisition \cite{meuleman_--fly_2025}. These methods make it possible to visualize a scene in real-time while it is being reconstructed, providing immediate feedback and enabling adaptive data acquisition.

Despite these advantages, online approaches typically estimate camera poses sequentially from image data alone. As a result, they are prone to drift and loop-closure errors, which can accumulate over time and lead to globally inconsistent reconstructions.

In parallel, Motion Capture (Mocap) systems offer an alternative means of camera pose estimation. By tracking a rigid body with a mounted camera, Mocap systems can provide camera poses with high accuracy and global consistency, independent of visual scene content. However, in the context of 3D reconstruction, such systems are rarely used as the primary source of camera pose retrieval. Instead, they are typically employed for benchmarking, validation, or ground truth generation, leaving their potential for direct integration into online reconstruction pipelines largely unexplored.

This thesis aims to fill this gap by introducing \emph{MocapGS}, a method that integrates accurate camera poses obtained from a Mocap system directly into an online 3D reconstruction pipeline based on 3DGS. To enable this integration, we propose a complete workflow consisting of:
\begin{enumerate}
\itemsep=0pt
\item Temporal Synchronization,
\item Camera Calibration,
\item Hand-eye Calibration,
\item Online 3D Reconstruction, and
\item 3D Mesh Extraction.
\end{enumerate}  

The main contributions of this work are threefold. First, we present the proposed workflow for integrating Mocap-based pose estimation into an online 3DGS reconstruction pipeline. Second, we evaluate the accuracy of the temporal synchronization and calibration procedures. Third, we assess the reconstruction quality and compare our approach against standard 3DGS as well as an online method that estimates camera poses solely from image data.

We find that by decoupling pose estimation from scene optimization, MocapGS combines the online visualization and incremental processing capabilities of modern 3DGS-based methods with the robustness and global consistency of Mocap-based camera tracking. 

\section{Related Work}\label{sec:related_work}
In this section, we summarize the related work regarding 3D reconstruction methods, as well as camera tracking with Mocap.

\subsection{3D Reconstruction Using Structure from Motion}\label{ssec:related_work:3d_reconstruction_sfm}
Classical 3D reconstruction pipelines are predominantly batch-based and follow a two-stage processing scheme. In a first stage, SfM is applied to a complete set of images to jointly estimate camera poses, intrinsics, and a sparse 3D point cloud via bundle adjustment.

In a second stage, dense reconstruction is performed using the estimated poses. Traditional photogrammetric methods rely on MVS to densify the sparse reconstruction. Neural Radiance Fields (NeRFs) \cite{mildenhall_nerf_2021}, a more recent learning-based approach, replaces explicit point-based reconstruction with a continuous scene representation, enabling high-quality novel view synthesis. 3DGS adopts a more explicit scene representation based on learned 3D Gaussian ellipsoids, enabling very fast rendering.

While highly accurate, these pipelines share a key limitation: SfM is always required as a preprocessing step and depends on the availability of the full image set. Consequently, such methods are non-incremental and do not support real-time reconstruction or interactive scene exploration.

\subsection{Online 3D Reconstruction}\label{ssec:related_work:online_3d_reconstruction}
Online 3D reconstruction methods aim to process image data sequentially as it becomes available, without prior knowledge of the full dataset. These approaches typically combine pose estimation and mapping in a single, incremental pipeline.

Classical point-based methods such as SLAM estimate camera poses on-the-fly while incrementally building a map of the environment. More recent learning-based methods extend this idea to neural or hybrid scene representations. Approaches such as MonoGS \cite{matsuki_gaussian_2024}, WildGS-SLAM \cite{zheng_wildgs-slam_2025}, or On-the-fly NVS \cite{meuleman_--fly_2025} enable incremental reconstruction and real-time rendering while estimating camera poses directly from incoming images.

A major advantage of online methods is the ability to visualize the scene in real time and to actively densify under-reconstructed regions. However, pose estimation is sequential and therefore depends on previously estimated poses. As a result, these methods are susceptible to drift and loop-closure errors, which can lead to globally inconsistent reconstructions.

\subsection{Camera Tracking with Motion Capture}\label{ssec:related_work:camera_tracking}
Mocap systems provide an alternative source of camera pose information. By rigidly mounting a camera to a set of tracked markers, the pose of the camera can be recovered in real-time with high accuracy after hand-eye calibration. Such systems yield globally consistent poses and are not affected by drift or loop-closure failures.

In the literature, Mocap is most commonly used for validation or ground truth generation rather than as the primary source of camera poses for reconstruction \cite{endres_evaluation_2012, sturm_benchmark_2012, shu_mocap2gt_2026}. 

This work is situated at the intersection of online 3D reconstruction and Mocap-based camera tracking. While online 3D reconstruction methods provide real-time feedback and incremental processing, Mocap offers accurate and globally consistent poses. The combination of these complementary properties motivates the approach presented in this work.

\section{Notation}\label{sec:notation}
This work extensively relies on rigid transformations in three-dimensional space. A rigid transformation in $\mathbb{R}^3$ has six degrees of freedom, comprising three rotational and three translational components. Throughout this paper, we refer to such transformations as \emph{poses}.

Following the standard formulation in projective geometry~\cite{hartley_multiple_2004}, poses are represented by homogeneous transformation matrices of the form
\begin{equation}
    \Matrix{E} =
    \begin{pmatrix}
        \Matrix{R} & \Vector{t} \\
        \mathbf{0}\trans & 1
    \end{pmatrix},
    \quad \Matrix{E} \in \mathbb{R}^{4 \times 4},
\end{equation}
where $\Matrix{R} \in \mathbb{R}^{3 \times 3}$ denotes a rotation matrix and $\Vector{t} \in \mathbb{R}^3$ a translation vector. Throughout this paper, bold, capital, monospaced symbols (e.g., $\Matrix{E}$, $\Matrix{T}$) are used to denote homogeneous transformation matrices representing poses.

The mapping from an arbitrary pose parameterization $\Vector{e}$ (e.g. Euler angles) to its associated homogeneous transformation matrix is denoted by $\Matrix{T}(\Vector{e})$.

To describe the relative pose between two coordinate systems, the notation $\TMatrix{A}{T}{B}$ is used. This denotes the pose of coordinate system $\mathrm{B}$ relative to coordinate system $\mathrm{A}$.

\section{Methodology}\label{sec:methodology}
This section provides a comprehensive overview of the methodology employed in this work. It systematically describes the proposed workflow for performing online 3D reconstruction using a synchronized and calibrated camera–Mocap system. Figure~\ref{fig:methodology_overview} illustrates the complete pipeline. Each component of the workflow is explained in detail throughout the subsequent sections.

\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{figures/methodology_overview.pdf}
    \caption[Overview of the proposed workflow]{Overview of the proposed workflow. The calibration process yields the latency offset $\Delta t$, the intrinsic parameter vector $\Vector{i}$, as well as the hand-eye pose $\TMatrix{C}{T}{T}$. These quantities then, together with the online acquired images and tool poses $\TMatrix{T}{T}{B}$, enable the 3DGS-based online 3D reconstruction.}
	\label{fig:methodology_overview}
\end{figure*}

\subsection{Definition of a Rigid Body}\label{ssec:methodology:rigid_body}
To enable camera pose tracking with a Mocap system, we attach a camera rigidly to a fixed set of Mocap markers, which is depicted in Figure~\ref{fig:rigid_body}. These markers define a rigid body that can be tracked by the Mocap system. To maintain the analogy to robotics, in the following, we refer to the rigid body as \emph{tool}. The pose of this tool coordinate system (TCS) can continously be tracked by the Mocap system. It differs from the pose of the camera coordinate system (CCS), that we are interested in, by an unknown pose $\TMatrix{C}{T}{T}$, which we call \emph{hand-eye pose}.

\begin{figure}[]
    \centering
    \includegraphics[width=\linewidth]{figures/rigid_body.png}
    \caption{Hand-held rigid body connecting the camera to spatially fixed set of Mocap markers.}
    \label{fig:rigid_body}
\end{figure}

\subsection{Temporal Synchronization}\label{ssec:methodology:temporal_sync}
To ensure temporally matching image-pose pairs, it is essential to synchronize the camera and the Mocap system precisely within the employed computer system. In general, such synchronization can be achieved using either hardware-based or software-based triggering mechanisms. To avoid the additional complexity and overhead of dedicated trigger cabling, a software-based synchronization approach is adopted in this work. 

Our goal therefore is to determine the latency difference $\Delta t$ between the camera and the Mocap system that arises on the workstation connected to both systems. To achieve this, we define an event that is temporally well localized and observable by both the camera and the Mocap system. By measuring the arrival times of this event on the workstation, the latency difference $\Delta t$ can be determined.

Common synchronization events for heterogeneous sensor setups include LED flashes or abrupt motions. However, the temporal resolution of such events is fundamentally limited by the camera frame rate, as they can typically only be detected as present or absent within individual image frames. This limits the achievable temporal precision.

To overcome this limitation, we instead exploit the continuous vertical motion of a Mocap marker as a synchronization signal. The marker's position is directly available from the Mocap data and can also be reliably segmented in the camera stream. From both modalities, we extract time series describing the marker's vertical movement: the vertical coordinate from the Mocap system and the row coordinate of the segmented marker center in the image sequence. We then fit a cubic spline to each time series to obtain a smooth, continuous representation of the motion. By determining the local maxima of the interpolated trajectories, we estimate the precise time points at which the marker reaches its maximum height with sub-frame accuracy. These time instants are subsequently used as high-precision synchronization events.

\subsection{Camera Calibration}\label{ssec:methodology:cc}
After temporally synchronizing the two systems, the camera calibration is performed. This involves estimating the camera's intrinsic parameter vector $\Vector{i}$, as well as the extrinsic parameters for each calibration image $\Vector{e}_i$. We perform camera calibration using multiple images of a checkerboard, which defines a world coordinate system (WCS). The extrinsic parameters $\Vector{e}_i$ to be estimated for each image describe the pose of the WCS relative to the CCS, so the corresponding homogenous transform is denoted as $\left(\TMatrix{C}{T}{W}\right)_i=\Matrix{T}\left(\Vector{e}_i\right)$. Our camera calibration procedure is based on OpenCV~\cite{bradski_g_opencv_2000} and internally consists of an initial parameter estimation followed by a nonlinear refinement.

\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{figures/transformations.pdf}
    \caption[]{Depiction of our MocapGS setup: The capture volume is depicted by the cuboid with the Mocap tracking cameras at the top. Left shows the relevant coordinate systems and right the relevant transformations between them.}
	\label{fig:transformations}
\end{figure*}

\paragraph{Initial Parameter Estimation.}
For estimating initial parameters, the method of Zhang~\cite{zhang_flexible_2000} is used. Zhang models the projection of a 3D point in the WCS onto the 2D Image Coordinate System (ICS) through the pinhole camera. This allows for a linear model formulation using homogenous coordinates. The model parameters, namely the intrinsic parameters of the pinhole camera $\Vector{i} = \left(f_x, f_y, c_x, c_y\right)$ and the extrinsic parameters of each image $\Vector{e}_i$, can then linearly be determined.

\paragraph{Nonlinear Refinemant.}
In the following stage, the linearly estimated parameters are refined via nonlinear optimization. Therefore, the camera model is not constrained to linearity and can thus be extended to incorporate lens distortions. We choose a polynomial camera model using two radial distortion coefficients $K_1$ and $K_2$ and two decentering distortion coefficients $P_1$ and $P_2$. 

Collecting all parameters, we define the vector
\begin{equation}
    \boldsymbol{\theta} = \left(f_x, f_y, c_x, c_y, K_1, K_2, P_1, P_2, \Vector{e}_1, \dots, \Vector{e}_{n_o}\right),
\end{equation}
which contains all unknowns to be optimized. The distortion parameters $K_1, K_2, P_1$, and $P_2$ are typically small, so we assume initial values of zero. We can now define the residual function
\begin{equation}
    \label{eq:repr_error_ics}
    d(\boldsymbol{\theta}) = \sum_{k=1}^{n_o} \sum_{j=1}^{n_m}
        \left\| \Vector{m}_{j,k} - \boldsymbol{\pi}\!\left( \Vector{M}_j, \boldsymbol{\theta} \right) \right\|_2^2 
        \;\longrightarrow\; \mathrm{min},
\end{equation}
where $\Vector{m}_{j,k}$ denotes the observed 2D ICS coordinate of the $j$-th corner in the $k$-th image. $\Vector{M}_j$ denotes the corresponding 3D WCS coordinate, which is projected into the ICS with the projection $\boldsymbol{\pi}$. The function $d(\boldsymbol{\theta})$ is commonly referred to as the reprojection error, which we minimize using the Levenberg-Marquardt method~\cite{marquardt_algorithm_1963}.

\subsection{Hand-eye Calibration}\label{ssec:methodology:hec}
During the acquisition of the calibration dataset, we not only capture calibration images but also record the corresponding tool pose from the Mocap system for each image, synchronized via $\Delta t$. This allows us to perform a hand-eye calibration.

Hand-eye calibration refers to the determination of the relative pose between a robot's end effector (tool) and a camera that is rigidly mounted to it~\cite{daniilidis_hand-eye_1999}. Establishing this relationship allows observations made in the CCS to be transformed into the TCS and subsequently into the robot base coordinate system (BCS).

In our case, we want to use hand-eye calbration to estimate the relative pose bewtween the CCS and the TCS, which can be tracked with the Mocap system. This hand-eye pose $\TMatrix{C}{T}{T}$ then allows the tool poses to be corrected, retrieving the camera pose which can then be used for the online 3D reconstruction. Figure~\ref{fig:transformations} illustrates the relevant coordinate systems and transformations in this context. As for the camera calibration, our hand-eye calibration procedure is based on OpenCV and consists of an initial parameter estimation followed by a nonlinear refinement.

\paragraph{Initial Parameter Estimation.}
The linear approach for estimating initial parameters is based on solving the fundamental equation
\begin{equation}
    \label{eq:hec_basic}
    \Matrix{A}\Matrix{X} = \Matrix{X}\Matrix{B}
\end{equation}
for the hand-eye pose $\Matrix{X}=\TMatrix{C}{T}{T}$. \Matrix{A} represents the known motion of the tool between two tool poses $\left(\TMatrix{T}{T}{B}\right)_i$ and $\left(\TMatrix{T}{T}{B}\right)_j$, which are obtained from the Mocap system. \Matrix{B} represents the corresponding known motion between two camera poses $\left(\TMatrix{C}{T}{W}\right)_i$ and $\left(\TMatrix{C}{T}{W}\right)_j$, which are obtained by the preceding camera calibration.

Several methods exist to solve Equation~\ref{eq:hec_basic}. We choose the approach proposed by Daniilidis~\cite{daniilidis_hand-eye_1999}, which uses dual quaternions to represent poses.

For the following nonlinear refinement, we also need an initial estimate of the base-world pose $\Matrix{Y}=\TMatrix{W}{T}{B}$. A reasonable initialization for $\Matrix{Y}$ can be derived by a single pair of tool pose and camera pose using
\begin{equation}
    \Matrix{Y} = \left(\TMatrix{C}{T}{W}\right)_i^{-1}\Matrix{X}\left(\TMatrix{T}{T}{B}\right)_i,
\end{equation}
which can be seen in Figure~\ref{fig:transformations}.

\paragraph{Nonlinear Refinement}
As for the camera calibration, we adopt a method that minimizes a reprojection error. In contrast to camera calibration, we transform a checkerboard point over a transformation chain containing the base-world pose, the tool pose, and the hand-eye pose before projecting it into the ICS. This allows optimizing for the hand-eye pose and base-world pose. We define the parameter vectors
\begin{equation}
    \begin{split}
        \Vector{x} &= \left( \alpha_x, \beta_x, \gamma_x, t_{x,x}, t_{y,x}, t_{z,x} \right),\\
        \Vector{y} &= \left( \alpha_y, \beta_y, \gamma_y, t_{x,y}, t_{y,y}, t_{z,y} \right),
    \end{split}
\end{equation}
which represent a minimal parameterization of the hand-eye pose~$\Matrix{X}=\TMatrix{C}{T}{T}$ and the base-world pose~$\Matrix{Y}=\TMatrix{W}{T}{B}$, respectively. These are the parameters to be optimized. The nonlinear refinement is formulated as the minimization of the reprojection error function
\begin{equation}
    \begin{split}
        \label{eq:hec_repr_error_ics}
        d(\Vector{x}, \Vector{y})
            &= \sum_{k=1}^{n_o} \sum_{j=1}^{n_m}
                \left\|
                    \Vector{m}_{j,k} -
                    \boldsymbol{\pi}\!\left(
                        \Vector{M}_{j,k}, \Vector{i}
                    \right)
                \right\|_2^2
            \;\longrightarrow\; \min, \\
        \Vector{M}_{j,k} &= \Matrix{T}(\Vector{x})\left(\TMatrix{T}{T}{B}\right)_k\Matrix{T}(\Vector{y})^{-1}
                        \Vector{M}_j,
    \end{split}
\end{equation}
where $\Vector{m}_{j,k}$ denotes the observed 2D image coordinate of the $j$-th calibration point in the $k$-th image. $\Vector{M}_j$ is the corresponding homogenous 3D WCS coordinate, which is transformed to the CCS of the $k$-th image through the transformation chain illustrated in Figure~\ref{fig:transformations}, yielding $\Vector{M}_{j,k}$. $\Vector{M}_{j,k}$ is then projected into the image via the projection function~$\boldsymbol{\pi}$ using the intrinsic camera parameters~\Vector{i}. The objective function~\eqref{eq:hec_repr_error_ics} is minimized using the Levenberg-Marquardt algorithm~\cite{marquardt_algorithm_1963}, yielding optimal estimates for both the hand-eye pose~\Matrix{X} and the base-world pose~\Matrix{Y}.

\subsection{Online 3D Reconstruction}\label{ssec:methodology:online_3d_reconstruction}
At this stage, the following quantities are available from the preceding synchronization and calibration procedure:
\begin{enumerate}
    \item the latency difference $\Delta t$,
    \item the intrinsic parameter vector $\Vector{i}$,
    \item the hand-eye pose $\TMatrix{C}{T}{T}$.
\end{enumerate}
With these quantities, we can now perform the 3DGS-based online 3D reconstruction. During the incremental reconstruction stage, $\Delta t$ enables us to continuously recieve temporally matched pairs of an image and its corresponding tool pose $\TMatrix{T}{T}{B}$. The obtained image is then, using the previously determined intrinsic parameter vector $\Vector{i}$, undistorted and shifted so that the principle point lies in the image center. In the same time, the tool pose $\TMatrix{T}{T}{B}$ can, togehter with the previously determined hand-eye pose $\TMatrix{C}{T}{T}$, be composed to recieve the desired pose of the BCS with respect to the CCS
\begin{equation}
    \label{eq:methodology:camera_pose_chain}
    \TMatrix{C}{T}{B} = \TMatrix{C}{T}{T} \TMatrix{T}{T}{B}.
\end{equation}
The shifted and undistorted image can then, together with its pose $\TMatrix{C}{T}{B}$, be used as an incremental input to the 3DGS-based online 3D reconstruction to perform reconstruction directly in the BCS.

As a baseline for our reconstruction algorithm, we use the method On-the-fly NVS~\cite{meuleman_--fly_2025}. This approach already handles incremental 3DGS, as well as initial camera pose and intrinsic parameter estimation and their refinement in the 3DGS optimization.

Our goal therefore is to streamline this baseline approach by eliminating the initial camera pose and intrinsic parameter estimation and replacing those by our well-known intrinsic parameter vector $\Vector{i}$ and on-the-fly acquired camera pose $\TMatrix{C}{T}{B}$. This enables us to perform online, incremental 3D reconstruction using 3DGS, already in the metric BCS. 

To investigate whether additional pose refinement is beneficial, we implement and evaluate three different variants. We refer to these methods as follows:

\begin{enumerate}
    \item \textbf{MocapGS-Fixed:} The initial camera poses obtained from the Mocap system are used directly and are not further refined.
    \item \textbf{MocapGS-FullOpt:} The initial camera poses are refined jointly with the Gaussian parameters during the 3DGS optimization.
    \item \textbf{MocapGS-RotOpt:} Only the rotational components of the initial camera poses are refined during 3DGS optimization, while the translational components are kept fixed in order to preserve the metric scale of the reconstruction.
\end{enumerate}

\subsection{3D Mesh Extraction}\label{ssec:methodology:mesh_extraction}
After completing the online 3DGS-based 3D reconstruction, we perform an offline surface extraction step to recover the explicit geometry of the observed scene or object. For this purpose, we adopt the efficient mesh extraction pipeline proposed in SuGaR~\cite{guedon_sugar_2024}, which is straightforward to integrate with a previously trained Gaussian representation.

SuGaR extracts surface points by identifying locations of equal density within the scene. Specifically, multiple depth maps are rendered from different viewpoints, and pixels are sampled from each depth map. For every corresponding camera ray, the method determines whether a point with a predefined density value exists along the ray and, if so, computes its 3D position. The collection of these points yields a sparse sampling of the underlying surface. Finally, Poisson surface reconstruction~\cite{kazhdan_poisson_2006} is applied to the extracted surface points to generate a triangular mesh.

\section{Experiments}\label{sec:experiments}
\subsection{Evaluation of the Calibration Accuracy}\label{sec:experiments:calibration}
\paragraph{Camera Calibration.}
\paragraph{Hand-Eye Calibration.}
\subsection{Evaluation of the Reconstruction Quality}\label{sec:experiments:reconstruction}
\paragraph{2D Evaluation.}
\paragraph{3D Evaluation.}

\section{Discussion}\label{sec:discussion}

\section{Conclusion}\label{sec:conclusion}

% The Appendices part is started with the command \appendix;
% appendix sections are then done as normal sections
%\appendix

%% Bibliography style file
\bibliographystyle{cas-model2-names}

% Bibliography database
\bibliography{references}

\end{document}

