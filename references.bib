
@article{kerbl_3d_2023,
	title = {{3D} {Gaussian} {Splatting} for {Real}-{Time} {Radiance} {Field} {Rendering}},
	volume = {42},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3592433},
	doi = {10.1145/3592433},
	abstract = {Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (≥ 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.},
	language = {en},
	number = {4},
	urldate = {2025-05-16},
	journal = {ACM Transactions on Graphics},
	author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkuehler, Thomas and Drettakis, George},
	month = aug,
	year = {2023},
	pages = {1--14},
	file = {PDF:/home/tkapler/Zotero/storage/2Z5RZ75K/Kerbl et al. - 2023 - 3D Gaussian Splatting for Real-Time Radiance Field Rendering.pdf:application/pdf},
}

@book{hartley_multiple_2004,
	address = {Cambridge},
	edition = {2},
	title = {Multiple {View} {Geometry} in {Computer} {Vision}},
	isbn = {978-0-521-54051-3},
	url = {https://www.cambridge.org/core/books/multiple-view-geometry-in-computer-vision/0B6F289C78B2B23F596CAA76D3D43F7A},
	doi = {10.1017/CBO9780511811685},
	abstract = {A basic problem in computer vision is to understand the structure of a real world scene given several images of it. Techniques for solving this problem are taken from projective geometry and photogrammetry. Here, the authors cover the geometric principles and their algebraic representation in terms of camera projection matrices, the fundamental matrix and the trifocal tensor. The theory and methods of computation of these entities are discussed with real examples, as is their use in the reconstruction of scenes from multiple images. The new edition features an extended introduction covering the key ideas in the book (which itself has been updated with additional examples and appendices) and significant new results which have appeared since the first edition. Comprehensive background material is provided, so readers familiar with linear algebra and basic numerical methods can understand the projective geometry and estimation algorithms presented, and implement the algorithms directly from the book.},
	urldate = {2025-11-01},
	publisher = {Cambridge University Press},
	author = {Hartley, Richard and Zisserman, Andrew},
	year = {2004},
	file = {Snapshot:/home/tkapler/Zotero/storage/Z45FRKYB/0B6F289C78B2B23F596CAA76D3D43F7A.html:text/html},
}

@article{zhang_flexible_2000,
	title = {A flexible new technique for camera calibration},
	volume = {22},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/document/888718},
	doi = {10.1109/34.888718},
	abstract = {We propose a flexible technique to easily calibrate a camera. It only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one more step from laboratory environments to real world use.},
	number = {11},
	urldate = {2025-11-01},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhang, Z.},
	month = nov,
	year = {2000},
	keywords = {Cameras, Calibration, Closed-form solution, Computer simulation, Computer vision, Layout, Lenses, Maximum likelihood estimation, Nonlinear distortion, Testing},
	pages = {1330--1334},
	file = {Snapshot:/home/tkapler/Zotero/storage/PWT3UIEJ/888718.html:text/html},
}

@book{steger_machine_2018,
	address = {Weinheim},
	edition = {2nd, completely revised and enlarged edition},
	title = {Machine vision algorithms and applications},
	isbn = {978-3-527-81290-5},
	language = {eng},
	publisher = {Wiley-VCH},
	editor = {Steger, Carsten and Ulrich, Markus and Wiedemann, Christian},
	year = {2018},
	keywords = {Dreidimensionale Bildverarbeitung, Maschinelles Sehen, Mustererkennung},
}

@inproceedings{faugeras_camera_1992,
	address = {Berlin, Heidelberg},
	title = {Camera self-calibration: {Theory} and experiments},
	isbn = {978-3-540-47069-4},
	shorttitle = {Camera self-calibration},
	doi = {10.1007/3-540-55426-2_37},
	abstract = {The problem of finding the internal orientation of a camera (camera calibration) is extremely important for practical applications. In this paper a complete method for calibrating a camera is presented. In contrast with existing methods it does not require a calibration object with a known 3D shape. The new method requires only point matches from image sequences. It is shown, using experiments with noisy data, that it is possible to calibrate a camera just by pointing it at the environment, selecting points of interest and then tracking them in the image as the camera moves. It is not necessary to know the camera motion.},
	language = {en},
	booktitle = {Computer {Vision} — {ECCV}'92},
	publisher = {Springer},
	author = {Faugeras, O. D. and Luong, Q. T. and Maybank, S. J.},
	editor = {Sandini, G.},
	year = {1992},
	keywords = {Camera Calibration, Camera Motion, Fundamental Matrix, Intrinsic Parameter, Polynomial System},
	pages = {321--334},
	file = {Full Text PDF:/home/tkapler/Zotero/storage/9J5FBWXX/Faugeras et al. - 1992 - Camera self-calibration Theory and experiments.pdf:application/pdf},
}

@article{bradski_g_opencv_2000,
	title = {The {OpenCV} {Library}},
	journal = {Dr. Dobb's Journal of Software Tools},
	author = {Bradski, G.},
	year = {2000},
}

@article{marquardt_algorithm_1963,
	title = {An {Algorithm} for {Least}-{Squares} {Estimation} of {Nonlinear} {Parameters}},
	volume = {11},
	issn = {0368-4245},
	url = {https://www.jstor.org/stable/2098941},
	number = {2},
	journal = {Journal of the Society for Industrial and Applied Mathematics},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Marquardt, Donald W.},
	year = {1963},
	pages = {431--441},
}

@inproceedings{schonberger_structure--motion_2016,
	title = {Structure-from-{Motion} {Revisited}},
	issn = {1063-6919},
	url = {https://ieeexplore.ieee.org/document/7780814},
	doi = {10.1109/CVPR.2016.445},
	abstract = {Incremental Structure-from-Motion is a prevalent strategy for 3D reconstruction from unordered image collections. While incremental reconstruction systems have tremendously advanced in all regards, robustness, accuracy, completeness, and scalability remain the key problems towards building a truly general-purpose pipeline. We propose a new SfM technique that improves upon the state of the art to make a further step towards this ultimate goal. The full reconstruction pipeline is released to the public as an open-source implementation.},
	urldate = {2025-11-07},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Schönberger, Johannes L. and Frahm, Jan-Michael},
	month = jun,
	year = {2016},
	keywords = {Cameras, Image reconstruction, Image registration, Internet, Pipelines, Robustness, Transmission line matrix methods},
	pages = {4104--4113},
	file = {Snapshot:/home/tkapler/Zotero/storage/GBWK2VYT/7780814.html:text/html},
}

@inproceedings{schonberger_pixelwise_2016,
	address = {Cham},
	title = {Pixelwise {View} {Selection} for {Unstructured} {Multi}-{View} {Stereo}},
	isbn = {978-3-319-46487-9},
	doi = {10.1007/978-3-319-46487-9_31},
	abstract = {This work presents a Multi-View Stereo system for robust and efficient dense modeling from unstructured image collections. Our core contributions are the joint estimation of depth and normal information, pixelwise view selection using photometric and geometric priors, and a multi-view geometric consistency term for the simultaneous refinement and image-based depth and normal fusion. Experiments on benchmarks and large-scale Internet photo collections demonstrate state-of-the-art performance in terms of accuracy, completeness, and efficiency.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Schönberger, Johannes L. and Zheng, Enliang and Frahm, Jan-Michael and Pollefeys, Marc},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Reprojection Error, Scene Representation, Source Image, Source Patch, View Selection},
	pages = {501--518},
	file = {Full Text PDF:/home/tkapler/Zotero/storage/NJDKDC3K/Schönberger et al. - 2016 - Pixelwise View Selection for Unstructured Multi-View Stereo.pdf:application/pdf},
}

@article{daniilidis_hand-eye_1999,
	title = {Hand-{Eye} {Calibration} {Using} {Dual} {Quaternions}},
	volume = {18},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/02783649922066213},
	doi = {10.1177/02783649922066213},
	abstract = {To relate measurements made by a sensor mounted on a mechanical link to the robot’s coordinate frame, we must first estimate the transformation between these two frames. Many algorithms have been proposed for this so-called hand-eye calibration, but they do not treat the relative position and orientation in a unified way. In this paper, we introduce the use of dual quaternions, which are the algebraic counterpart of screws. Then we show how a line transformation can be written with the dual-quaternion product. We algebraically prove that if we consider the camera and motor transformations as screws, then only the line coefficients of the screw axes are relevant regarding the hand-eye calibration. The dual-quaternion parameterization facilitates a new simultaneous solution for the hand-eye rotation and translation using the singular value decomposition. Real-world performance is assessed directly in the application of hand-eye information for stereo reconstruction, as well as in the positioning of cameras. Both real and synthetic experiments show the superiority of the approach over two other proposed methods.},
	language = {EN},
	number = {3},
	urldate = {2025-11-10},
	journal = {The International Journal of Robotics Research},
	publisher = {SAGE Publications Ltd STM},
	author = {Daniilidis, Konstantinos},
	month = mar,
	year = {1999},
	pages = {286--298},
	file = {SAGE PDF Full Text:/home/tkapler/Zotero/storage/R3YRYAD3/Daniilidis - 1999 - Hand-Eye Calibration Using Dual Quaternions.pdf:application/pdf},
}

@article{ulrich_uncertainty-aware_2024,
	title = {Uncertainty-{Aware} {Hand}–{Eye} {Calibration}},
	volume = {40},
	issn = {1941-0468},
	url = {https://ieeexplore.ieee.org/abstract/document/10310118/citations},
	doi = {10.1109/TRO.2023.3330609},
	abstract = {We provide a generic framework for the hand–eye calibration of vision-guided industrial robots. In contrast to traditional methods, we explicitly model the uncertainty of the robot in a stochastically founded way. Albeit the repeatability of modern industrial robots is high, their absolute accuracy typically is much lower. This uncertainty—especially if not considered—deteriorates the result of the hand–eye calibration. Our proposed framework does not only result in a high accuracy of the computed hand–eye pose but also provides reliable information about the uncertainty of the robot. It further provides corrected robot poses for a convenient and inexpensive robot calibration. Our framework is computationally efficient and generic in several regards. It supports the use of a calibration target as well as self-calibration without the need for known 3-D points. It optionally enables the simultaneous calibration of the interior camera parameters. The framework is also generic with regard to the robot type and, hence, supports antropomorphic as well as selective compliance assembly robot arm (SCARA) robots, for example. Simulated and real experiments show the validity of the proposed methods. An extensive evaluation of our framework on a public dataset shows a considerably higher accuracy than 15 state-of-the-art methods.},
	urldate = {2025-11-11},
	journal = {IEEE Transactions on Robotics},
	author = {Ulrich, Markus and Hillemann, Markus},
	year = {2024},
	keywords = {Cameras, Three-dimensional displays, Calibration, Automation, calibration, computer vision, hand-eye calibration, industrial robots, measurement uncertainty, Robot kinematics, Robot vision systems, Robots, Uncertainty},
	pages = {573--591},
}

@inproceedings{zwicker_ewa_2001,
	title = {{EWA} volume splatting},
	url = {https://ieeexplore.ieee.org/abstract/document/964490},
	doi = {10.1109/VISUAL.2001.964490},
	abstract = {In this paper we present a novel framework for direct volume rendering using a splatting approach based on elliptical Gaussian kernels. To avoid aliasing artifacts, we introduce the concept of a resampling filter combining a reconstruction with a low-pass kernel. Because of the similarity to Heckbert's EWA (elliptical weighted average) filter for texture mapping we call our technique EWA volume splatting. It provides high image quality without aliasing artifacts or excessive blurring even with non-spherical kernels. Hence it is suitable for regular, rectilinear, and irregular volume data sets. Moreover, our framework introduces a novel approach to compute the footprint function. It facilitates efficient perspective projection of arbitrary elliptical kernels at very little additional cost. Finally, we show that EWA volume reconstruction kernels can be reduced to surface reconstruction kernels. This makes our splat primitive universal in reconstructing surface and volume data.},
	urldate = {2025-11-12},
	booktitle = {Proceedings {Visualization}, 2001. {VIS} '01.},
	author = {Zwicker, M. and Pfister, H. and van Baar, J. and Gross, M.},
	month = oct,
	year = {2001},
	keywords = {Rendering (computer graphics), Surface fitting},
	pages = {29--538},
	file = {Snapshot:/home/tkapler/Zotero/storage/YJK7EKVA/964490.html:text/html},
}

@inproceedings{merrill_revisiting_2010,
	title = {Revisiting sorting for {GPGPU} stream architectures},
	url = {https://dl.acm.org/doi/10.1145/1854273.1854344},
	doi = {10.1145/1854273.1854344},
	language = {en},
	urldate = {2025-11-13},
	booktitle = {Proceedings of the 19th international conference on {Parallel} architectures and compilation techniques},
	publisher = {ACM},
	author = {Merrill, Duane G. and Grimshaw, Andrew S.},
	month = sep,
	year = {2010},
	pages = {545--546},
}

@inproceedings{guedon_sugar_2024,
	title = {{SuGaR}: {Surface}-{Aligned} {Gaussian} {Splatting} for {Efficient} {3D} {Mesh} {Reconstruction} and {High}-{Quality} {Mesh} {Rendering}},
	issn = {2575-7075},
	shorttitle = {{SuGaR}},
	url = {https://ieeexplore.ieee.org/document/10655755},
	doi = {10.1109/CVPR52733.2024.00512},
	abstract = {We propose a method to allow precise and extremely fast mesh extraction from 3D Gaussian Splatting [15]. Gaussian Splatting has recently become very popular as it yields realistic rendering while being significantly faster to train than NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D Gaussians as these Gaussians tend to be unorganized after optimization and no method has been proposed so far. Our first key contribution is a regularization term that encourages the Gaussians to align well with the surface of the scene. We then introduce a method that exploits this alignment to extract a mesh from the Gaussians using Poisson reconstruction, which is fast, scalable, and preserves details, in contrast to the Marching Cubes algorithm usually applied to extract meshes from Neural SDFs. Finally, we introduce an optional refinement strategy that binds Gaussians to the surface of the mesh, and jointly optimizes these Gaussians and the mesh through Gaussian splatting rendering. This enables easy editing, sculpting, animating, and relighting of the Gaussians by manipulating the mesh instead of the Gaussians themselves. Retrieving such an editable mesh for realistic rendering is done within minutes with our method, compared to hours with the state-of-the-art method on SDFs, while providing a better rendering quality.},
	urldate = {2025-12-21},
	booktitle = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Guédon, Antoine and Lepetit, Vincent},
	month = jun,
	year = {2024},
	keywords = {Rendering (computer graphics), Three-dimensional displays, Computer vision, 3D from Multi-view and Sensors, Differentiable Rendering, Gaussian Splatting, Geometry, Harmonic analysis, Mesh, Neural radiance field, Novel View Synthesis, Surface reconstruction},
	pages = {5354--5363},
	file = {Snapshot:/home/tkapler/Zotero/storage/ASMQ9HF7/10655755.html:text/html;Submitted Version:/home/tkapler/Zotero/storage/R9NL97FH/Guédon and Lepetit - 2024 - SuGaR Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh.pdf:application/pdf},
}

@inproceedings{kazhdan_poisson_2006,
	address = {Goslar, DEU},
	series = {{SGP} '06},
	title = {Poisson surface reconstruction},
	isbn = {978-3-905673-36-4},
	doi = {10.5555/1281957.1281965},
	abstract = {We show that surface reconstruction from oriented points can be cast as a spatial Poisson problem. This Poisson formulation considers all the points at once, without resorting to heuristic spatial partitioning or blending, and is therefore highly resilient to data noise. Unlike radial basis function schemes, our Poisson approach allows a hierarchy of locally supported basis functions, and therefore the solution reduces to a well conditioned sparse linear system. We describe a spatially adaptive multiscale algorithm whose time and space complexities are proportional to the size of the reconstructed model. Experimenting with publicly available scan data, we demonstrate reconstruction of surfaces with greater detail than previously achievable.},
	urldate = {2025-12-21},
	booktitle = {Proceedings of the fourth {Eurographics} symposium on {Geometry} processing},
	publisher = {Eurographics Association},
	author = {Kazhdan, Michael and Bolitho, Matthew and Hoppe, Hugues},
	month = jun,
	year = {2006},
	pages = {61--70},
}

@misc{ids_imaging_development_systems_gmbh_ids_2025,
	title = {{IDS} {Peak} {SDK}},
	url = {https://en.ids-imaging.com/ids-peak.html},
	author = {{IDS Imaging Development Systems GmbH}},
	year = {2025},
}

@misc{naturalpoint_inc_motive_2025,
	title = {Motive},
	url = {https://www.optitrack.com/software/motive/},
	author = {{NaturalPoint, Inc.}},
	year = {2025},
}

@misc{naturalpoint_inc_natnet_2025,
	title = {{NatNet} {SDK}},
	url = {https://optitrack.com/software/natnet-sdk},
	author = {{NaturalPoint, Inc.}},
	year = {2025},
}

@article{petrovska_vision_2024,
	title = {Vision through {Obstacles}—{3D} {Geometric} {Reconstruction} and {Evaluation} of {Neural} {Radiance} {Fields} ({NeRFs})},
	volume = {16},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/16/7/1188},
	doi = {10.3390/rs16071188},
	abstract = {In this contribution we evaluate the 3D geometry reconstructed by Neural Radiance Fields (NeRFs) of an object’s occluded parts behind obstacles through a point cloud comparison in 3D space against traditional Multi-View Stereo (MVS), addressing the accuracy and completeness. The key challenge lies in recovering the underlying geometry, completing the occluded parts of the object and investigating if NeRFs can compete against traditional MVS for scenarios where the latter falls short. In addition, we introduce a new “obSTaclE, occLusion and visibiLity constrAints” dataset named STELLA concerning transparent and non-transparent obstacles in real-world scenarios since there is no existing dataset dedicated to this problem setting to date. Considering that the density field represents the 3D geometry of NeRFs and is solely position-dependent, we propose an effective approach for extracting the geometry in the form of a point cloud. We voxelize the whole density field and apply a 3D density-gradient based Canny edge detection filter to better represent the object’s geometric features. The qualitative and quantitative results demonstrate NeRFs’ ability to capture geometric details of the occluded parts in all scenarios, thus outperforming in completeness, as our voxel-based point cloud extraction approach achieves point coverage up to 93\%. However, MVS remains a more accurate image-based 3D reconstruction method, deviating from the ground truth 2.26 mm and 3.36 mm for each obstacle scenario respectively.},
	language = {en},
	number = {7},
	urldate = {2025-12-31},
	journal = {Remote Sensing},
	publisher = {Multidisciplinary Digital Publishing Institute},
	author = {Petrovska, Ivana and Jutzi, Boris},
	month = jan,
	year = {2024},
	keywords = {3D reconstruction, geometry evaluation, multi-view stereo, neural radiance fields, new dataset, obstacles, point clouds},
	pages = {1188},
	file = {Full Text PDF:/home/tkapler/Zotero/storage/6T9U3W9A/Petrovska and Jutzi - 2024 - Vision through Obstacles—3D Geometric Reconstruction and Evaluation of Neural Radiance Fields (NeRFs.pdf:application/pdf},
}

@misc{cloudcompare_project_cloudcompare_2025,
	title = {{CloudCompare}},
	copyright = {GNU General Public License (GPL)},
	url = {https://www.cloudcompare.org/},
	author = {{CloudCompare Project}},
	year = {2025},
}

@article{besl_method_1992,
	title = {A method for registration of 3-{D} shapes},
	volume = {14},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/document/121791},
	doi = {10.1109/34.121791},
	abstract = {The authors describe a general-purpose, representation-independent method for the accurate and computationally efficient registration of 3-D shapes including free-form curves and surfaces. The method handles the full six degrees of freedom and is based on the iterative closest point (ICP) algorithm, which requires only a procedure to find the closest point on a geometric entity to a given point. The ICP algorithm always converges monotonically to the nearest local minimum of a mean-square distance metric, and the rate of convergence is rapid during the first few iterations. Therefore, given an adequate set of initial rotations and translations for a particular class of objects with a certain level of 'shape complexity', one can globally minimize the mean-square distance metric over all six degrees of freedom by testing each initial registration. One important application of this method is to register sensed data from unfixtured rigid objects with an ideal geometric model, prior to shape inspection. Experimental results show the capabilities of the registration algorithm on point sets, curves, and surfaces.{\textless}{\textgreater}},
	number = {2},
	urldate = {2025-12-31},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Besl, P.J. and McKay, Neil D.},
	month = feb,
	year = {1992},
	keywords = {Testing, Convergence, Inspection, Iterative algorithms, Iterative closest point algorithm, Iterative methods, Motion estimation, Quaternions, Shape measurement, Solid modeling},
	pages = {239--256},
	file = {Snapshot:/home/tkapler/Zotero/storage/YPYKI93V/121791.html:text/html},
}

@article{mildenhall_nerf_2021,
	title = {{NeRF}: representing scenes as neural radiance fields for view synthesis},
	volume = {65},
	issn = {0001-0782},
	shorttitle = {{NeRF}},
	url = {https://dl.acm.org/doi/10.1145/3503250},
	doi = {10.1145/3503250},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ, ϕ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.},
	number = {1},
	urldate = {2026-01-22},
	journal = {Commun. ACM},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	month = dec,
	year = {2021},
	pages = {99--106},
	file = {Full Text:/home/tkapler/Zotero/storage/4JWEN4WP/Mildenhall et al. - 2021 - NeRF representing scenes as neural radiance fields for view synthesis.pdf:application/pdf},
}

@article{cadena_past_2016,
	title = {Past, {Present}, and {Future} of {Simultaneous} {Localization} and {Mapping}: {Toward} the {Robust}-{Perception} {Age}},
	volume = {32},
	issn = {1941-0468},
	shorttitle = {Past, {Present}, and {Future} of {Simultaneous} {Localization} and {Mapping}},
	url = {https://ieeexplore.ieee.org/abstract/document/7747236},
	doi = {10.1109/TRO.2016.2624754},
	abstract = {Simultaneous localization and mapping (SLAM) consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications and witnessing a steady transition of this technology to industry. We survey the current state of SLAM and consider future directions. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?},
	number = {6},
	urldate = {2026-01-22},
	journal = {IEEE Transactions on Robotics},
	author = {Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, José and Reid, Ian and Leonard, John J.},
	month = dec,
	year = {2016},
	keywords = {Robustness, Factor graphs, Graph theory, localization, Localization, mapping, maximum a posteriori estimation, perception, robots, sensing, Service robots, simultaneous localization and mapping (SLAM), Simultaneous location and mapping},
	pages = {1309--1332},
	file = {Snapshot:/home/tkapler/Zotero/storage/6DTPS5PK/7747236.html:text/html;Submitted Version:/home/tkapler/Zotero/storage/UMTA8C66/Cadena et al. - 2016 - Past, Present, and Future of Simultaneous Localization and Mapping Toward the Robust-Perception Age.pdf:application/pdf},
}

@article{shu_mocap2gt_2026,
	title = {{MoCap2GT}: {A} {High}-{Precision} {Ground} {Truth} {Estimator} for {SLAM} {Benchmarking} {Based} on {Motion} {Capture} and {IMU} {Fusion}},
	volume = {11},
	issn = {2377-3766},
	shorttitle = {{MoCap2GT}},
	url = {https://ieeexplore.ieee.org/document/11297812},
	doi = {10.1109/LRA.2025.3643287},
	abstract = {Marker-based optical motion capture (MoCap) systems are widely used to provide ground truth (GT) trajectories for benchmarking SLAM algorithms. However, the accuracy of MoCap-based GT trajectories is mainly affected by two factors: spatiotemporal calibration errors between the MoCap system and the device under test (DUT), and inherent MoCap jitter. Consequently, existing benchmarks focus primarily on absolute translation error, as accurate assessment of rotation and inter-frame errors remains challenging, hindering thorough SLAM evaluation. This letter proposes MoCap2GT, a joint optimization approach that integrates MoCap data and inertial measurement unit (IMU) measurements from the DUT for generating high-precision GT trajectories. MoCap2GT includes a robust state initializer to ensure global convergence, introduces a higher-order B-spline pose parameterization on the SE(3) manifold with a varying time offset to effectively model MoCap factors, and employs a degeneracy-aware measurement rejection strategy to enhance estimation accuracy. Experimental results demonstrate that MoCap2GT outperforms existing methods and significantly contributes to precise SLAM benchmarking.},
	number = {2},
	urldate = {2026-01-22},
	journal = {IEEE Robotics and Automation Letters},
	author = {Shu, Zichao and Bei, Shitao and Dai, Jicheng and Li, Lijun and Chen, Zetao and Wang, Jianyu},
	month = feb,
	year = {2026},
	keywords = {Accuracy, Cameras, Simultaneous localization and mapping, Calibration, Benchmark testing, calibration and identification, Estimation, Jitter, Performance evaluation and benchmarking, Sensors, Spatiotemporal phenomena, Trajectory, visual-inertial SLAM},
	pages = {1538--1545},
	file = {Snapshot:/home/tkapler/Zotero/storage/TDXINST8/11297812.html:text/html},
}

@article{gomes_3d_2014,
	series = {Depth {Image} {Analysis}},
	title = {{3D} reconstruction methods for digital preservation of cultural heritage: {A} survey},
	volume = {50},
	issn = {0167-8655},
	shorttitle = {{3D} reconstruction methods for digital preservation of cultural heritage},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865514001032},
	doi = {10.1016/j.patrec.2014.03.023},
	abstract = {3D reconstruction, refers to capturing and reproducing the shape and appearance of an arbitrary object or scene given depth and color information. This is a broad research area within the computer vision field involving many stages and still open problems. The digital preservation of cultural heritage is a specially challenging application of 3D reconstruction. Cultural heritage objects and sites greatly differ from each other and a maximized fidelity of the 3D reconstruction is a core requirement. The literature on this topic has substantially increased in the past years, mostly due to the variety of scenarios and the development of new depth sensing devices as well as techniques able to deal with this issue. In our search to develop a complete 3D reconstruction pipeline, we have comprehensively studied techniques related to this topic and divided the 3D digitization process in four major overviews: image acquisition, view registration, mesh integration and texture generation. We present the state-of-the-art approaches and challenges of each stage.},
	urldate = {2026-01-22},
	journal = {Pattern Recognition Letters},
	author = {Gomes, Leonardo and Regina Pereira Bellon, Olga and Silva, Luciano},
	month = dec,
	year = {2014},
	keywords = {3D reconstruction, Cultural heritage, Depth image, Digital preservation, Survey},
	pages = {3--14},
	file = {ScienceDirect Snapshot:/home/tkapler/Zotero/storage/DMPPN3UH/S0167865514001032.html:text/html},
}

@inproceedings{behley_efficient_2018,
	title = {Efficient {Surfel}-{Based} {SLAM} using {3D} {Laser} {Range} {Data} in {Urban} {Environments}},
	isbn = {978-0-9923747-4-7},
	url = {http://www.roboticsproceedings.org/rss14/p16.pdf},
	doi = {10.15607/RSS.2018.XIV.016},
	abstract = {Accurate and reliable localization and mapping is a fundamental building block for most autonomous robots. For this purpose, we propose a novel, dense approach to laserbased mapping that operates on three-dimensional point clouds obtained from rotating laser sensors. We construct a surfel-based map and estimate the changes in the robot’s pose by exploiting the projective data association between the current scan and a rendered model view from that surfel map. For detection and veriﬁcation of a loop closure, we leverage the map representation to compose a virtual view of the map before a potential loop closure, which enables a more robust detection even with low overlap between the scan and the already mapped areas. Our approach is efﬁcient and enables real-time capable registration. At the same time, it is able to detect loop closures and to perform map updates in an online fashion. Our experiments show that we are able to estimate globally consistent maps in large scale environments solely based on point cloud data.},
	language = {en},
	urldate = {2026-01-24},
	booktitle = {Robotics: {Science} and {Systems} {XIV}},
	publisher = {Robotics: Science and Systems Foundation},
	author = {Behley, Jens and Stachniss, Cyrill},
	month = jun,
	year = {2018},
	file = {PDF:/home/tkapler/Zotero/storage/9BXNE94L/Behley and Stachniss - 2018 - Efficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments.pdf:application/pdf},
}

@article{meuleman_--fly_2025,
	title = {On-the-fly {Reconstruction} for {Large}-{Scale} {Novel} {View} {Synthesis} from {Unposed} {Images}},
	volume = {44},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3730913},
	doi = {10.1145/3730913},
	abstract = {Radiance field methods such as 3D Gaussian Splatting (3DGS) allow easy reconstruction from photos, enabling free-viewpoint navigation. Nonetheless, pose estimation using Structure from Motion and 3DGS optimization can still each take between minutes and hours of computation after capture is complete. SLAM methods combined with 3DGS are fast but struggle with wide camera baselines and large scenes. We present an on-the-fly method to produce camera poses and a trained 3DGS
              immediately
              after capture. Our method can handle dense and wide-baseline captures of ordered photo sequences and large-scale scenes. To do this, we first introduce fast initial pose estimation, exploiting learned features and a GPU-friendly mini bundle adjustment. We then introduce direct sampling of Gaussian primitive positions and shapes, incrementally spawning primitives where required, significantly accelerating training. These two efficient steps allow fast and robust joint optimization of poses and Gaussian primitives. Our incremental approach handles large-scale scenes by introducing scalable radiance field construction, progressively clustering 3DGS primitives, storing them in anchors, and offloading them from the GPU. Clustered primitives are progressively merged, keeping the required scale of 3DGS at any viewpoint. We evaluate our solution on a variety of datasets and show that it can provide on-the-fly processing of all the capture scenarios and scene sizes we target. At the same time our method remains competitive - in speed, image quality, or both - with other methods that only handle specific capture styles or scene sizes.},
	language = {en},
	number = {4},
	urldate = {2026-01-25},
	journal = {ACM Transactions on Graphics},
	author = {Meuleman, Andreas and Shah, Ishaan and Lanvin, Alexandre and Kerbl, Bernhard and Drettakis, George},
	month = aug,
	year = {2025},
	pages = {1--14},
	file = {PDF:/home/tkapler/Zotero/storage/D36TNY3L/Meuleman et al. - 2025 - On-the-fly Reconstruction for Large-Scale Novel View Synthesis from Unposed Images.pdf:application/pdf},
}

@inproceedings{matsuki_gaussian_2024,
	address = {Seattle, WA, USA},
	title = {Gaussian {Splatting} {SLAM}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-5300-6},
	url = {https://ieeexplore.ieee.org/document/10657715/},
	doi = {10.1109/CVPR52733.2024.01708},
	language = {en},
	urldate = {2026-01-25},
	booktitle = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Matsuki, Hidenobu and Murai, Riku and Kelly, Paul H. J. and Davison, Andrew J.},
	month = jun,
	year = {2024},
	pages = {18039--18048},
	file = {PDF:/home/tkapler/Zotero/storage/WICYRHWL/Matsuki et al. - 2024 - Gaussian Splatting SLAM.pdf:application/pdf},
}

@article{denavit_kinematic_1955,
	title = {A {Kinematic} {Notation} for {Lower}-{Pair} {Mechanisms} {Based} on {Matrices}},
	volume = {22},
	issn = {0021-8936, 1528-9036},
	url = {https://asmedigitalcollection.asme.org/appliedmechanics/article/22/2/215/1110292/A-Kinematic-Notation-for-Lower-Pair-Mechanisms},
	doi = {10.1115/1.4011045},
	abstract = {Abstract
            A symbolic notation devised by Reuleaux to describe mechanisms did not recognize the necessary number of variables needed for complete description. A reconsideration of the problem leads to a symbolic notation which permits the complete description of the kinematic properties of all lower-pair mechanisms by means of equations. The symbolic notation also yields a method for studying lower-pair mechanisms by means of matrix algebra; two examples of application to space mechanisms are given.},
	language = {en},
	number = {2},
	urldate = {2026-01-25},
	journal = {Journal of Applied Mechanics},
	author = {Denavit, J. and Hartenberg, R. S.},
	month = jun,
	year = {1955},
	pages = {215--221},
	file = {PDF:/home/tkapler/Zotero/storage/7X982G84/Denavit and Hartenberg - 1955 - A Kinematic Notation for Lower-Pair Mechanisms Based on Matrices.pdf:application/pdf},
}

@inproceedings{endres_evaluation_2012,
	address = {St Paul, MN, USA},
	title = {An evaluation of the {RGB}-{D} {SLAM} system},
	isbn = {978-1-4673-1405-3},
	url = {http://ieeexplore.ieee.org/document/6225199/},
	doi = {10.1109/ICRA.2012.6225199},
	abstract = {We present an approach to simultaneous localization and mapping (SLAM) for RGB-D cameras like the Microsoft Kinect. Our system concurrently estimates the trajectory of a hand-held Kinect and generates a dense 3D model of the environment. We present the key features of our approach and evaluate its performance thoroughly on a recently published dataset, including a large set of sequences of different scenes with varying camera speeds and illumination conditions. In particular, we evaluate the accuracy, robustness, and processing time for three different feature descriptors (SIFT, SURF, and ORB). The experiments demonstrate that our system can robustly deal with difﬁcult data in common indoor scenarios while being fast enough for online operation. Our system is fully available as open-source.},
	language = {en},
	urldate = {2026-01-25},
	booktitle = {2012 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	publisher = {IEEE},
	author = {Endres, Felix and Hess, Jurgen and Engelhard, Nikolas and Sturm, Jurgen and Cremers, Daniel and Burgard, Wolfram},
	month = may,
	year = {2012},
	pages = {1691--1696},
	file = {PDF:/home/tkapler/Zotero/storage/585X2Q3W/Endres et al. - 2012 - An evaluation of the RGB-D SLAM system.pdf:application/pdf},
}

@inproceedings{sturm_benchmark_2012,
	address = {Vilamoura-Algarve, Portugal},
	title = {A benchmark for the evaluation of {RGB}-{D} {SLAM} systems},
	isbn = {978-1-4673-1736-8},
	url = {http://ieeexplore.ieee.org/document/6385773/},
	doi = {10.1109/IROS.2012.6385773},
	abstract = {In this paper, we present a novel benchmark for the evaluation of RGB-D SLAM systems. We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 × 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an ofﬁce environment and an industrial hall. The dataset covers a large variety of scenes and camera motions. We provide sequences for debugging with slow motions as well as longer trajectories with and without loop closures. Most sequences were recorded from a handheld Kinect with unconstrained 6-DOF motions but we also provide sequences from a Kinect mounted on a Pioneer 3 robot that was manually navigated through a cluttered indoor environment. To stimulate the comparison of different approaches, we provide automatic evaluation tools both for the evaluation of drift of visual odometry systems and the global pose error of SLAM systems. The benchmark website [1] contains all data, detailed descriptions of the scenes, speciﬁcations of the data formats, sample code, and evaluation tools.},
	language = {en},
	urldate = {2026-01-25},
	booktitle = {2012 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	publisher = {IEEE},
	author = {Sturm, Jrgen and Engelhard, Nikolas and Endres, Felix and Burgard, Wolfram and Cremers, Daniel},
	month = oct,
	year = {2012},
	pages = {573--580},
	file = {PDF:/home/tkapler/Zotero/storage/XHRS2YWT/Sturm et al. - 2012 - A benchmark for the evaluation of RGB-D SLAM systems.pdf:application/pdf},
}

@inproceedings{zheng_wildgs-slam_2025,
	title = {{WildGS}-{SLAM}: {Monocular} {Gaussian} {Splatting} {SLAM} in {Dynamic} {Environments}},
	shorttitle = {{WildGS}-{SLAM}},
	language = {en},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zheng, Jianhao and Zhu, Zihan and Bieri, Valentin and Pollefeys, Marc and Peng, Songyou and Armeni, Iro},
	year = {2025},
	pages = {11461--11471},
	file = {Full Text PDF:/home/tkapler/Zotero/storage/WS8K8YAR/Zheng et al. - 2025 - WildGS-SLAM Monocular Gaussian Splatting SLAM in Dynamic Environments.pdf:application/pdf},
}

@article{petrovska_geometric_2023,
	title = {Geometric {Accuracy} {Analysis} between {Neural} {Radiance} {Fields} ({NeRFs}) and {Terrestrial} {Laser} {Scanning} ({TLS})},
	volume = {XLVIII-1-W3-2023},
	issn = {1682-1750},
	url = {https://isprs-archives.copernicus.org/articles/XLVIII-1-W3-2023/153/2023/},
	doi = {10.5194/isprs-archives-XLVIII-1-W3-2023-153-2023},
	abstract = {Neural Radiance Fields (NeRFs) use a set of camera poses with associated images to represent a scene through a position-dependent density and radiance at given spatial location. Generating a geometric representation in form of a point cloud is gained by ray tracing and sampling 3D points with density and color along the rays. In this contribution we evaluate object reconstruction by NeRFs in 3D metric space against Terrestrial Laser Scanning (TLS) using ground truth data in form of a Structured Light Imaging (SLI) mesh and investigate the influence of the density to the reconstruction\&rsquo;s accuracy. We extend the accuracy assessment from 2D to 3D space and perform high resolution investigations on NeRFs by using camera images with 36MP resolution as well as comparison among point clouds of more than 20 million points against a 0.1mm ground truth mesh. TLS achieves the highest geometric accuracy results with a standard deviation of 1.68mm, while NeRF\&delta;t=300 diverges 18.61mm from the ground truth. All NeRF reconstructions contain 3D points inside the object which have the highest displacements from the ground truth, thus contribute the most to the accuracy results. NeRFs accuracy improves with increasing the density threshold as a consequence of completeness, since beside noise and outliers the object points are also being removed.},
	language = {English},
	urldate = {2026-01-25},
	journal = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	publisher = {Copernicus GmbH},
	author = {Petrovska, I. and Jäger, M. and Haitz, D. and Jutzi, B.},
	month = oct,
	year = {2023},
	note = {Conference Name: 2nd GEOBENCH Workshop on Evaluation and BENCHmarking of Sensors, Systems and GEOspatial Data in Photogrammetry and Remote Sensing - 23–24 October 2023, Krakow, Poland},
	keywords = {3D Reconstruction, Accuracy Assessment, Laser Scanning, Neural Radiance Fields, Point Cloud Comparison},
	pages = {153--159},
	file = {Full Text PDF:/home/tkapler/Zotero/storage/BNXAMPSD/Petrovska et al. - 2023 - GEOMETRIC ACCURACY ANALYSIS BETWEEN NEURAL RADIANCE FIELDS (NERFS) AND TERRESTRIAL LASER SCANNING (T.pdf:application/pdf},
}

@inproceedings{seitz_comparison_2006,
	title = {A {Comparison} and {Evaluation} of {Multi}-{View} {Stereo} {Reconstruction} {Algorithms}},
	volume = {1},
	issn = {1063-6919},
	url = {https://ieeexplore.ieee.org/document/1640800/},
	doi = {10.1109/CVPR.2006.19},
	abstract = {This paper presents a quantitative comparison of several multi-view stereo reconstruction algorithms. Until now, the lack of suitable calibrated multi-view image datasets with known ground truth (3D shape models) has prevented such direct comparisons. In this paper, we first survey multi-view stereo algorithms and compare them qualitatively using a taxonomy that differentiates their key properties. We then describe our process for acquiring and calibrating multiview image datasets with high-accuracy ground truth and introduce our evaluation methodology. Finally, we present the results of our quantitative comparison of state-of-the-art multi-view stereo reconstruction algorithms on six benchmark datasets. The datasets, evaluation details, and instructions for submitting new models are available online at http://vision.middlebury.edu/mview.},
	urldate = {2026-01-26},
	booktitle = {2006 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'06)},
	author = {Seitz, S.M. and Curless, B. and Diebel, J. and Scharstein, D. and Szeliski, R.},
	month = jun,
	year = {2006},
	note = {ISSN: 1063-6919},
	keywords = {Cameras, Layout, Image reconstruction, Shape measurement, Educational institutions, Image databases, Reconstruction algorithms, Stereo image processing, Stereo vision, Taxonomy},
	pages = {519--528},
	file = {Full Text PDF:/home/tkapler/Zotero/storage/VHT6SD4A/Seitz et al. - 2006 - A Comparison and Evaluation of Multi-View Stereo Reconstruction Algorithms.pdf:application/pdf},
}

@article{yang_depth_2024,
	title = {Depth {Anything} {V2}},
	volume = {37},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/26cfdcd8fe6fd75cc53e92963a656c58-Abstract-Conference.html},
	doi = {10.52202/079017-0688},
	language = {en},
	urldate = {2026-01-27},
	journal = {Advances in Neural Information Processing Systems},
	author = {Yang, Lihe and Kang, Bingyi and Huang, Zilong and Zhao, Zhen and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
	month = dec,
	year = {2024},
	pages = {21875--21911},
	file = {Full Text PDF:/home/tkapler/Zotero/storage/C7DX8N72/Yang et al. - 2024 - Depth Anything V2.pdf:application/pdf},
}
